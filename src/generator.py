"""
Generator - LLM ile YanÄ±t Ãœretme ModÃ¼lÃ¼

Bu modÃ¼l, retrieval sonuÃ§larÄ±nÄ± kullanarak LLM ile yanÄ±t Ã¼retir.

RAG Sistemindeki RolÃ¼:
- Retrieved context'i LLM'e verir
- KullanÄ±cÄ± sorusunu yanÄ±tlar
- Prompt engineering yapar

KullanÄ±lan Teknoloji: Ollama
- Lokal LLM Ã§alÄ±ÅŸtÄ±rma
- Ãœcretsiz ve gizlilik dostu
- Ã‡eÅŸitli model desteÄŸi (llama, mistral vb.)

Prompt Template Ã–nemli!
- Sisteme rol/talimat verir
- Context'i doÄŸru formatta sunar
- YanÄ±t kalitesini etkiler
"""

from typing import Optional, Dict, Any


class Generator:
    """
    LLM YanÄ±t Ãœretici

    Ollama API'si ile lokal LLM kullanarak
    context-based yanÄ±tlar Ã¼retir.

    Ã–rnek kullanÄ±m:
    >>> generator = Generator(model="llama3.2")
    >>> response = generator.generate(
    ...     question="Python nedir?",
    ...     context="Python yÃ¼ksek seviyeli bir programlama dilidir..."
    ... )
    >>> print(response)
    """

    # VarsayÄ±lan prompt template (context ile)
    DEFAULT_TEMPLATE = """Sen yardÄ±mcÄ± bir asistansÄ±n. Sana verilen baÄŸlam bilgisini kullanarak soruyu yanÄ±tla.

KURALLAR:
1. SADECE verilen baÄŸlam bilgisini kullan
2. BaÄŸlamda olmayan bilgiyi uydurma
3. Emin deÄŸilsen "Bu konuda bilgim yok" de
4. YanÄ±tÄ± TÃ¼rkÃ§e ver
5. KÄ±sa ve Ã¶z ol

BAÄLAM:
{context}

SORU: {question}

YANIT:"""

    # Context olmadan sohbet template
    CHAT_TEMPLATE = """Sen yardÄ±mcÄ± bir TÃ¼rkÃ§e asistansÄ±n. KullanÄ±cÄ±nÄ±n mesajÄ±na kÄ±sa ve samimi yanÄ±t ver.

SORU: {question}

YANIT:"""

    def __init__(
        self,
        model: str = "llama3.2",
        template: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 500,
    ):
        """
        Args:
            model: Ollama model adÄ± (llama3.2, mistral, vb.)
            template: Ã–zel prompt template (opsiyonel)
            temperature: YaratÄ±cÄ±lÄ±k seviyesi (0-1)
            max_tokens: Maksimum yanÄ±t uzunluÄŸu
        """
        self.model = model
        self.template = template or self.DEFAULT_TEMPLATE
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.client = None

        self._initialize_client()

    def _initialize_client(self):
        """Ollama client'Ä±nÄ± baÅŸlatÄ±r."""
        try:
            import ollama

            self.client = ollama
            print(f"âœ… Ollama baÄŸlantÄ±sÄ± hazÄ±r (model: {self.model})")
        except ImportError:
            print("âš ï¸  ollama paketi bulunamadÄ±!")
            print("   Kurulum: pip install ollama")
            print("   AyrÄ±ca Ollama uygulamasÄ±nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.")
            self.client = None

    def generate(self, question: str, context: str, stream: bool = False) -> str:
        """
        Soru ve context'e gÃ¶re yanÄ±t Ã¼retir.

        Args:
            question: KullanÄ±cÄ± sorusu
            context: Retrieved bilgi (chunk'lar)
            stream: Streaming modu (opsiyonel)

        Returns:
            LLM yanÄ±tÄ±
        """
        if not self.client:
            return self._fallback_response(question, context)

        # Context varsa RAG template, yoksa sohbet template kullan
        if context and context.strip():
            prompt = self.template.format(context=context, question=question)
        else:
            prompt = self.CHAT_TEMPLATE.format(question=question)

        print(f"ğŸ¤– LLM yanÄ±t Ã¼retiyor ({self.model})...")

        try:
            if stream:
                return self._generate_stream(prompt)
            else:
                return self._generate_sync(prompt)

        except Exception as e:
            print(f"âŒ LLM hatasÄ±: {e}")
            return self._fallback_response(question, context)

    def _generate_sync(self, prompt: str) -> str:
        """Senkron yanÄ±t Ã¼retimi."""
        response = self.client.generate(
            model=self.model,
            prompt=prompt,
            options={"temperature": self.temperature, "num_predict": self.max_tokens},
        )

        return response["response"].strip()

    def _generate_stream(self, prompt: str) -> str:
        """Streaming yanÄ±t Ã¼retimi."""
        full_response = ""

        for chunk in self.client.generate(
            model=self.model,
            prompt=prompt,
            stream=True,
            options={"temperature": self.temperature, "num_predict": self.max_tokens},
        ):
            text = chunk["response"]
            print(text, end="", flush=True)
            full_response += text

        print()  # Yeni satÄ±r
        return full_response.strip()

    def _fallback_response(self, question: str, context: str) -> str:
        """
        Ollama Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nda basit fallback yanÄ±t.

        Bu, sistemin Ã§alÄ±ÅŸmasÄ±nÄ± test etmek iÃ§in kullanÄ±lÄ±r.
        GerÃ§ek projede LLM aktif olmalÄ±.
        """
        print("âš ï¸  Fallback mod: LLM olmadan basit yanÄ±t")

        if not context:
            return "ÃœzgÃ¼nÃ¼m, bu konuda bilgi bulamadÄ±m."

        # Basit extractive yanÄ±t: Ä°lk context parÃ§asÄ±nÄ± dÃ¶ndÃ¼r
        first_chunk = context.split("---")[0].strip()
        if "[Kaynak" in first_chunk:
            # Kaynak etiketini kaldÄ±r
            lines = first_chunk.split("\n")
            first_chunk = "\n".join(lines[1:]).strip()

        return f"BulduÄŸum bilgiye gÃ¶re:\n\n{first_chunk[:500]}..."

    def check_model_available(self) -> bool:
        """Model'in Ollama'da yÃ¼klÃ¼ olup olmadÄ±ÄŸÄ±nÄ± kontrol eder."""
        if not self.client:
            return False

        try:
            models = self.client.list()
            model_names = [m["name"] for m in models.get("models", [])]

            # Model adÄ± veya model:tag formatÄ±nÄ± kontrol et
            for name in model_names:
                if self.model in name or name.startswith(self.model):
                    return True

            return False

        except Exception:
            return False

    def set_template(self, template: str):
        """Prompt template'i deÄŸiÅŸtirir."""
        self.template = template
        print("âœ… Prompt template gÃ¼ncellendi")


# FarklÄ± kullanÄ±m senaryolarÄ± iÃ§in template'ler
TEMPLATES = {
    "default": Generator.DEFAULT_TEMPLATE,
    "concise": """Verilen baÄŸlama gÃ¶re soruyu kÄ±saca yanÄ±tla.

BaÄŸlam: {context}

Soru: {question}

KÄ±sa YanÄ±t:""",
    "detailed": """Sen bir uzman asistansÄ±n. AÅŸaÄŸÄ±daki baÄŸlam bilgisini kullanarak 
soruyu detaylÄ± ve aÃ§Ä±klayÄ±cÄ± bir ÅŸekilde yanÄ±tla.

=== BAÄLAM ===
{context}

=== SORU ===
{question}

=== DETAYLI YANIT ===
""",
    "qa_with_sources": """Soruyu yanÄ±tla ve kaynaklarÄ±nÄ± belirt.

BaÄŸlam:
{context}

Soru: {question}

YanÄ±t (kaynaklarla birlikte):""",
}


# Test iÃ§in
if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("GENERATOR TEST")
    print("=" * 60)

    generator = Generator(model="llama3.2")

    # Model kontrolÃ¼
    if generator.check_model_available():
        print(f"âœ… Model mevcut: {generator.model}")

        # Test yanÄ±tÄ±
        test_context = """
        Python, Guido van Rossum tarafÄ±ndan geliÅŸtirilen yÃ¼ksek seviyeli 
        bir programlama dilidir. 1991'de ilk sÃ¼rÃ¼mÃ¼ yayÄ±nlanmÄ±ÅŸtÄ±r.
        OkunabilirliÄŸi ve basit sÃ¶zdizimi ile bilinir.
        """

        response = generator.generate(
            question="Python'u kim geliÅŸtirdi?", context=test_context
        )

        print(f"\nğŸ’¬ YanÄ±t:\n{response}")
    else:
        print(f"âš ï¸  Model bulunamadÄ±: {generator.model}")
        print("   Kurulum: ollama pull llama3.2")
