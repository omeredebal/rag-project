"""
Retriever - Bilgi Getirme ModÃ¼lÃ¼

Bu modÃ¼l, kullanÄ±cÄ± sorgusuna en uygun chunk'larÄ± getirir.

RAG Sistemindeki RolÃ¼:
- KullanÄ±cÄ± sorusunu anlama
- En alakalÄ± bilgileri bulma
- LLM'e context saÄŸlama

Retrieval SÃ¼reci:
1. Sorgu â†’ Embedding
2. Embedding â†’ Vector Store'da arama
3. Top-K en benzer chunk'larÄ± getir
4. SonuÃ§larÄ± sÄ±rala ve dÃ¶ndÃ¼r

Neden Retrieval Ã–nemli?
- LLM'in bilgi kapasitesi sÄ±nÄ±rlÄ±
- GÃ¼ncel/Ã¶zel bilgiler LLM'de yok
- Retrieval ile doÄŸru context â†’ DoÄŸru yanÄ±t
"""

from typing import List, Optional, Dict
from dataclasses import dataclass


@dataclass
class RetrievalResult:
    """Retrieval sonucunu temsil eden veri sÄ±nÄ±fÄ±"""

    content: str  # Chunk iÃ§eriÄŸi
    score: float  # Benzerlik skoru
    metadata: Dict  # Kaynak bilgisi

    def __repr__(self):
        preview = self.content[:60] + "..." if len(self.content) > 60 else self.content
        return f"RetrievalResult(score={self.score:.3f}, content='{preview}')"


class Retriever:
    """
    Bilgi Getirici

    KullanÄ±cÄ± sorgusuna gÃ¶re en alakalÄ± chunk'larÄ±
    vector store'dan getirir.

    Ã–rnek kullanÄ±m:
    >>> retriever = Retriever(embedder, vector_store)
    >>> results = retriever.retrieve("Python nedir?", top_k=3)
    >>> for r in results:
    >>>     print(f"Score: {r.score:.2f} - {r.content[:50]}...")
    """

    def __init__(
        self,
        embedder,  # Embedder instance
        vector_store,  # VectorStore instance
        top_k: int = 5,
        score_threshold: float = 0.0,
    ):
        """
        Args:
            embedder: Embedding oluÅŸturucu
            vector_store: VektÃ¶r deposu
            top_k: VarsayÄ±lan sonuÃ§ sayÄ±sÄ±
            score_threshold: Minimum skor eÅŸiÄŸi (altÄ±ndakiler filtrelenir)
        """
        self.embedder = embedder
        self.vector_store = vector_store
        self.top_k = top_k
        self.score_threshold = score_threshold

    def retrieve(
        self,
        query: str,
        top_k: Optional[int] = None,
        filter_metadata: Optional[Dict] = None,
    ) -> List[RetrievalResult]:
        """
        Sorguya en uygun chunk'larÄ± getirir.

        Args:
            query: KullanÄ±cÄ± sorusu
            top_k: DÃ¶ndÃ¼rÃ¼lecek sonuÃ§ sayÄ±sÄ±
            filter_metadata: Metadata filtresi

        Returns:
            RetrievalResult listesi (skora gÃ¶re sÄ±ralÄ±)
        """
        if not query or not query.strip():
            print("âš ï¸  BoÅŸ sorgu!")
            return []

        k = top_k or self.top_k

        print(f"ğŸ” AranÄ±yor: '{query[:50]}...' (top_k={k})")

        # 1. Sorguyu embed et
        query_embedding = self.embedder.embed_text(query)

        # 2. Vector store'da ara
        search_results = self.vector_store.search(
            query_embedding=query_embedding, top_k=k, where=filter_metadata
        )

        # 3. SonuÃ§larÄ± dÃ¶nÃ¼ÅŸtÃ¼r ve filtrele
        results = []
        for sr in search_results:
            print(f"   ğŸ“Š Skor: {sr.score:.3f} (eÅŸik: {self.score_threshold})")
            # Skor eÅŸiÄŸini kontrol et
            if sr.score >= self.score_threshold:
                results.append(
                    RetrievalResult(
                        content=sr.content, score=sr.score, metadata=sr.metadata
                    )
                )

        print(
            f"âœ… {len(results)} sonuÃ§ bulundu (filtrelendi: {len(search_results) - len(results)})"
        )

        return results

    def retrieve_with_context(
        self,
        query: str,
        top_k: Optional[int] = None,
        context_separator: str = "\n\n---\n\n",
    ) -> str:
        """
        SonuÃ§larÄ± birleÅŸtirilmiÅŸ context olarak dÃ¶ndÃ¼rÃ¼r.

        Bu metod, LLM'e verilecek context string'ini oluÅŸturur.

        Args:
            query: KullanÄ±cÄ± sorusu
            top_k: SonuÃ§ sayÄ±sÄ±
            context_separator: Chunk'lar arasÄ± ayÄ±rÄ±cÄ±

        Returns:
            BirleÅŸtirilmiÅŸ context string
        """
        results = self.retrieve(query, top_k)

        if not results:
            return ""

        # Chunk'larÄ± birleÅŸtir
        context_parts = []
        for i, result in enumerate(results, 1):
            source = result.metadata.get("filename", "Bilinmeyen")
            context_parts.append(f"[Kaynak {i}: {source}]\n{result.content}")

        return context_separator.join(context_parts)

    def get_sources(self, query: str, top_k: Optional[int] = None) -> List[str]:
        """
        SonuÃ§larÄ±n kaynaklarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.

        KullanÄ±cÄ±ya "Bu bilgi ÅŸu kaynaklardan geldi" demek iÃ§in.
        """
        results = self.retrieve(query, top_k)

        sources = []
        for r in results:
            source = r.metadata.get("source", r.metadata.get("filename", "Bilinmeyen"))
            if source not in sources:
                sources.append(source)

        return sources


class HybridRetriever:
    """
    Hibrit Retriever (Ä°leri Seviye)

    Semantic search + Keyword search birleÅŸtirir.
    Bu, basit projede opsiyoneldir.
    """

    def __init__(self, retriever: Retriever):
        self.retriever = retriever

    def retrieve(
        self, query: str, top_k: int = 5, keyword_weight: float = 0.3
    ) -> List[RetrievalResult]:
        """
        Semantic ve keyword aramayÄ± birleÅŸtirir.

        Not: Bu basitleÅŸtirilmiÅŸ bir implementasyon.
        GerÃ§ek hibrit arama iÃ§in BM25 + dense retrieval kullanÄ±lÄ±r.
        """
        # Åimdilik sadece semantic arama
        return self.retriever.retrieve(query, top_k)


# Test iÃ§in
if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("RETRIEVER TEST")
    print("=" * 60)
    print("Not: Bu test gerÃ§ek Embedder ve VectorStore gerektirir.")
    print("Demo.py dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rarak tam testi yapabilirsiniz.")
