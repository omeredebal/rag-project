"""
RAG Pipeline - Ana Sistem ModÃ¼lÃ¼

Bu modÃ¼l, tÃ¼m RAG bileÅŸenlerini birleÅŸtiren ana pipeline'dÄ±r.

RAG Pipeline AkÄ±ÅŸÄ±:
1. INDEXING (Bir kez yapÄ±lÄ±r)
   DÃ¶kÃ¼manlar â†’ Chunking â†’ Embedding â†’ Vector Store

2. QUERYING (Her soru iÃ§in)
   Soru â†’ Embedding â†’ Retrieval â†’ LLM â†’ YanÄ±t

Bu sÄ±nÄ±f, tÃ¼m adÄ±mlarÄ± orchestrate eder.
"""

import os
from typing import List, Optional, Dict
from dataclasses import dataclass

from .document_loader import DocumentLoader, Document
from .chunker import TextChunker, Chunk
from .embedder import Embedder
from .vector_store import VectorStore
from .retriever import Retriever, RetrievalResult
from .generator import Generator


@dataclass
class RAGResponse:
    """RAG yanÄ±tÄ±nÄ± temsil eden veri sÄ±nÄ±fÄ±"""

    answer: str  # LLM yanÄ±tÄ±
    sources: List[str]  # Kaynak dÃ¶kÃ¼manlar
    retrieved_chunks: List[RetrievalResult]  # Bulunan chunk'lar

    def __repr__(self):
        return (
            f"RAGResponse(answer='{self.answer[:50]}...', sources={len(self.sources)})"
        )


class RAGPipeline:
    """
    RAG Pipeline

    TÃ¼m RAG bileÅŸenlerini birleÅŸtiren ana sÄ±nÄ±f.
    End-to-end soru-cevap sistemi saÄŸlar.

    Ã–rnek kullanÄ±m:
    >>> rag = RAGPipeline()
    >>> rag.index_documents("data/")
    >>> response = rag.query("Python nedir?")
    >>> print(response.answer)
    """

    def __init__(
        self,
        # Embedding ayarlarÄ±
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        # Chunking ayarlarÄ±
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        # Vector store ayarlarÄ±
        collection_name: str = "rag_documents",
        persist_directory: str = "./chroma_db",
        # LLM ayarlarÄ±
        llm_model: str = "llama3.2",
        # Retrieval ayarlarÄ±
        top_k: int = 3,
    ):
        """
        RAG Pipeline'Ä± yapÄ±landÄ±rÄ±r.

        Args:
            embedding_model: Sentence transformer model adÄ±
            chunk_size: Chunk boyutu (karakter)
            chunk_overlap: Chunk Ã¶rtÃ¼ÅŸmesi (karakter)
            collection_name: ChromaDB koleksiyon adÄ±
            persist_directory: VektÃ¶r DB kayÄ±t dizini
            llm_model: Ollama model adÄ±
            top_k: Her sorgu iÃ§in dÃ¶ndÃ¼rÃ¼lecek chunk sayÄ±sÄ±
        """
        print("\n" + "=" * 60)
        print("ğŸš€ RAG PIPELINE BAÅLATILIYOR")
        print("=" * 60 + "\n")

        # KonfigÃ¼rasyonu sakla
        self.config = {
            "embedding_model": embedding_model,
            "chunk_size": chunk_size,
            "chunk_overlap": chunk_overlap,
            "collection_name": collection_name,
            "persist_directory": persist_directory,
            "llm_model": llm_model,
            "top_k": top_k,
        }

        # BileÅŸenleri baÅŸlat
        print("1ï¸âƒ£  Document Loader hazÄ±rlanÄ±yor...")
        self.loader = DocumentLoader()

        print("\n2ï¸âƒ£  Text Chunker hazÄ±rlanÄ±yor...")
        self.chunker = TextChunker(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

        print(f"\n3ï¸âƒ£  Embedder hazÄ±rlanÄ±yor ({embedding_model})...")
        self.embedder = Embedder(model_name=embedding_model)

        print(f"\n4ï¸âƒ£  Vector Store hazÄ±rlanÄ±yor ({collection_name})...")
        self.vector_store = VectorStore(
            collection_name=collection_name, persist_directory=persist_directory
        )

        print("\n5ï¸âƒ£  Retriever hazÄ±rlanÄ±yor...")
        self.retriever = Retriever(
            embedder=self.embedder, vector_store=self.vector_store, top_k=top_k
        )

        print(f"\n6ï¸âƒ£  Generator hazÄ±rlanÄ±yor ({llm_model})...")
        self.generator = Generator(model=llm_model)

        print("\n" + "=" * 60)
        print("âœ… RAG PIPELINE HAZIR!")
        print("=" * 60 + "\n")

    def index_documents(self, source: str, clear_existing: bool = False) -> int:
        """
        DÃ¶kÃ¼manlarÄ± indexler (embedding + vector store).

        Bu iÅŸlem bir kez yapÄ±lÄ±r. DÃ¶kÃ¼manlar vector store'a eklenir.

        Args:
            source: DÃ¶kÃ¼man yolu (dosya veya dizin)
            clear_existing: Mevcut verileri sil

        Returns:
            Ä°ndexlenen chunk sayÄ±sÄ±
        """
        print("\n" + "-" * 60)
        print("ğŸ“¥ INDEXING BAÅLIYOR")
        print("-" * 60)

        # Mevcut verileri temizle (opsiyonel)
        if clear_existing:
            print("\nğŸ§¹ Mevcut veriler temizleniyor...")
            self.vector_store.clear()

        # 1. DÃ¶kÃ¼manlarÄ± yÃ¼kle
        print("\n[AdÄ±m 1/4] DÃ¶kÃ¼manlar yÃ¼kleniyor...")

        if os.path.isfile(source):
            docs = [self.loader.load_file(source)]
            docs = [d for d in docs if d]  # None'larÄ± filtrele
        elif os.path.isdir(source):
            docs = self.loader.load_directory(source)
        else:
            # DoÄŸrudan metin olarak kabul et
            docs = [self.loader.load_text(source, "direct_input")]

        if not docs:
            print("âš ï¸  YÃ¼klenecek dÃ¶kÃ¼man bulunamadÄ±!")
            return 0

        # 2. Chunk'la
        print("\n[AdÄ±m 2/4] Metinler parÃ§alanÄ±yor (chunking)...")
        chunks = self.chunker.chunk_documents(docs)

        if not chunks:
            print("âš ï¸  OluÅŸturulan chunk yok!")
            return 0

        # 3. Embed et
        print("\n[AdÄ±m 3/4] Embedding'ler oluÅŸturuluyor...")
        contents = [chunk.content for chunk in chunks]
        embeddings = self.embedder.embed_texts(contents)

        # 4. Vector store'a ekle
        print("\n[AdÄ±m 4/4] Vector store'a ekleniyor...")
        count = self.vector_store.add_documents(chunks, embeddings)

        print("\n" + "-" * 60)
        print(f"âœ… INDEXING TAMAMLANDI: {count} chunk indexlendi")
        print("-" * 60 + "\n")

        return count

    def query(
        self, question: str, top_k: Optional[int] = None, return_sources: bool = True
    ) -> RAGResponse:
        """
        Soru sorar ve RAG ile yanÄ±t alÄ±r.

        Args:
            question: KullanÄ±cÄ± sorusu
            top_k: KullanÄ±lacak chunk sayÄ±sÄ±
            return_sources: Kaynak bilgisi ekle

        Returns:
            RAGResponse nesnesi
        """
        print("\n" + "-" * 60)
        print(f"â“ SORU: {question}")
        print("-" * 60)

        k = top_k or self.config["top_k"]

        # 1. Retrieval
        print("\n[AdÄ±m 1/2] Ä°lgili bilgiler aranÄ±yor...")
        retrieved = self.retriever.retrieve(question, top_k=k)

        if not retrieved:
            print("âš ï¸  Ä°lgili bilgi bulunamadÄ±!")
            return RAGResponse(
                answer="ÃœzgÃ¼nÃ¼m, bu konuda bilgi bulamadÄ±m.",
                sources=[],
                retrieved_chunks=[],
            )

        # Context'i oluÅŸtur
        context = self.retriever.retrieve_with_context(question, top_k=k)

        # 2. Generation
        print("\n[AdÄ±m 2/2] YanÄ±t Ã¼retiliyor...")
        answer = self.generator.generate(question=question, context=context)

        # KaynaklarÄ± topla
        sources = []
        if return_sources:
            sources = self.retriever.get_sources(question, top_k=k)

        print("\n" + "-" * 60)
        print("âœ… YANIT HAZIR")
        print("-" * 60 + "\n")

        return RAGResponse(answer=answer, sources=sources, retrieved_chunks=retrieved)

    def add_document(self, text: str, source_name: str = "manual_input") -> int:
        """
        Tek bir metni sisteme ekler.

        Args:
            text: Eklenecek metin
            source_name: Kaynak adÄ±

        Returns:
            Eklenen chunk sayÄ±sÄ±
        """
        doc = self.loader.load_text(text, source_name)
        chunks = self.chunker.chunk_documents([doc])

        contents = [chunk.content for chunk in chunks]
        embeddings = self.embedder.embed_texts(contents)

        return self.vector_store.add_documents(chunks, embeddings)

    def get_stats(self) -> Dict:
        """Sistem istatistiklerini dÃ¶ndÃ¼rÃ¼r."""
        return {
            "config": self.config,
            "vector_store": self.vector_store.get_stats(),
            "llm_available": self.generator.check_model_available(),
        }

    def clear(self):
        """TÃ¼m indexlenmiÅŸ verileri siler."""
        self.vector_store.clear()
        print("ğŸ§¹ TÃ¼m veriler temizlendi")


# HÄ±zlÄ± baÅŸlangÄ±Ã§ iÃ§in yardÄ±mcÄ± fonksiyon
def create_rag_pipeline(**kwargs) -> RAGPipeline:
    """
    RAG Pipeline oluÅŸturmak iÃ§in factory fonksiyonu.

    Ã–rnek:
    >>> rag = create_rag_pipeline(llm_model="mistral", top_k=5)
    """
    return RAGPipeline(**kwargs)


# Test iÃ§in
if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("RAG PIPELINE TEST")
    print("=" * 60)

    # Pipeline oluÅŸtur
    rag = RAGPipeline(
        persist_directory="./test_chroma_db", collection_name="test_collection"
    )

    # Test verisi ekle
    test_doc = """
    Python Programlama Dili
    
    Python, Guido van Rossum tarafÄ±ndan geliÅŸtirilen yÃ¼ksek seviyeli 
    bir programlama dilidir. Ä°lk sÃ¼rÃ¼mÃ¼ 1991'de yayÄ±nlanmÄ±ÅŸtÄ±r.
    
    Python'un Ã–zellikleri:
    - Okunabilir ve temiz sÃ¶zdizimi
    - Dinamik tip sistemi
    - GeniÅŸ standart kÃ¼tÃ¼phane
    - Ã‡oklu paradigma desteÄŸi (OOP, fonksiyonel)
    
    KullanÄ±m AlanlarÄ±:
    - Web geliÅŸtirme (Django, Flask)
    - Veri bilimi (NumPy, Pandas)
    - Yapay zeka (TensorFlow, PyTorch)
    - Otomasyon ve scripting
    """

    # Ä°ndexle
    rag.index_documents(test_doc, clear_existing=True)

    # Soru sor
    response = rag.query("Python'u kim geliÅŸtirdi?")

    print("\nğŸ’¬ YANIT:")
    print(response.answer)
    print(f"\nğŸ“š Kaynaklar: {response.sources}")
